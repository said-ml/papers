Attention Is All You Need (Vaswani et al., 2017)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)
GPT-3: Language Models are Few-Shot Learners (Brown et al., 2020)
XLNet: Generalized Autoregressive Pretraining for Language Understanding (Yang et al., 2019)
RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)
ALBERT: A Lite BERT for Self-attention Learning (Lan et al., 2019)
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter (Sanh et al., 2019)
BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Question Answering (Lewis et al., 2019)
T5: Text-to-Text Transfer Transformer (Raffel et al., 2019)
LaMDA: Language Models for Dialog Applications (Brown et al., 2022)
