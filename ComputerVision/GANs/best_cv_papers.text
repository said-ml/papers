Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (Ren et al., 2015)
Mask R-CNN (He et al., 2017)
YOLOv3: An Incremental Improvement (Redmon and Farhadi, 2018)
EfficientDet: Scalable and Efficient Object Detection (Tan et al., 2019)
Detectron2: A Unified Framework for Object Detection, Segmentation and Pose Estimation (Wu et al., 2019)
DeepLabv3+: Dilated ResNet for Semantic Image Segmentation (Chen et al., 2018)
U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)
SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers (Zhang et al., 2022)
ViT: An Image is Worth 16x16 Words: Vision Transformers for Image Classification, Detection and Segmentation (Dosovitskiy et al., 2021)
Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Liu et al., 2021)
Deformable ConvNets (Dai et al., 2017)
DCNv2: Improved Deformable Convolutional Networks for Visual Recognition (Zhu et al., 2019)
GCN: Graph Convolutional Networks (Kipf and Welling, 2016)
GAT: Graph Attention Networks (Velickovic et al., 2017)
MoCo: Momentum Contrast for Unsupervised Visual Representation Learning (He et al., 2020)
SimCLR: A Simple Framework for Contrastive Learning of Visual Representations (Chen et al., 2020)
BYOL: Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning (Grill et al., 2020)
DINO: DeiT with NO Supervision (Caron et al., 2021)
BEiT: BERT for Image Modeling (Dehghani et al., 2021)
Vision Perceptor: A Unified Approach to Vision and Language (Dosovitskiy et al., 2022)
ImageNet 64x64: A New Benchmark Architecture for Image Classification (Chen et al., 2022)
EfficientNet-Lite: A Mobile-Friendly Family of EfficientNets for Automatic Image Classification (Tan et al., 2020)
GhostNet: A New Ghost Module for Resource-Constrained Network Design (Han et al., 2019)
ShuffleNet: An Extremely Efficient Convolutional Neural Architecture for Mobile Computer Vision (Zhang et al., 2017)
SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size (Iandola et al., 2016)
